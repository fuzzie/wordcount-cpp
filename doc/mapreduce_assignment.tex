\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}
% \usepackage{url}
\usepackage{fullpage}
\usepackage{breakurl} 
\usepackage{hyperref}
\usepackage{listings}
\usepackage{color}
\usepackage{mathtools}
\usepackage[english]{babel}

% A modified version of the tutorial from https://github.com/skoulouzis/MapReduceAssignment14-15

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{ %
  language=Java,        
  basicstyle=\ttfamily\footnotesize,     
  backgroundcolor=\color{white},  
  showspaces=false,      
  showstringspaces=false,    
  showtabs=false,        
  frame=single,         
  rulecolor=\color{black},   
  tabsize=2,          
  captionpos=b,        
  breaklines=true,        
  title=\lstname,        
  keywordstyle=\color{blue},    
  commentstyle=\color{dkgreen},   
  stringstyle=\color{mauve},    
  escapeinside={\%*}{*)},     
  morekeywords={*,...} 
}



\date{}


\title{Hadoop Assignment\\
  Concurrency en Parallel Programmeren 2015,\\
  University of Amsterdam
}


\begin{document}
  \maketitle
  
  \tableofcontents
  
  \section{Overview}
  
  In this assignment, you have to analyze a dataset containing Twitter posts. In particular, you have to perform three tasks:

  \begin{enumerate}
    \item Find the top ten most popular hashtags.
    \item Perform sentiment analysis on the English-language tweets, and find the average and standard deviation of the sentiments for each of those top ten hashtags.
    \item Investigate retweet and reply behaviour.
  \end{enumerate}

  There is a small dataset (5k tweets) available on Blackboard. We also have larger datasets available in \texttt{/projects/cpp2015} on the cluster. Using them for your experiments is optional, but you might find them useful. Remember, though: the cluster is a shared resource. Don't run large numbers of unnecessary experiments!
 
  \section{Implementation}

  You should already have the framework from the tutorial; you should simply build on top of it, and implement these analysis tasks using MapReduce with Hadoop.
  
  Remember: first, develop your code locally, and make sure it works there, and then make sure it works on the cluster, where you should run your experiments
  for the report.
%  After you have studied and understood the tutorial use the code framework to implement the twitter dataset analysis. To clone the code from github use:
%  \lstset{language=} 
%  \begin{lstlisting}
%    $ git clone https://github.com/skoulouzis/MapReduceAssignment14-15.git
%  \end{lstlisting}
%  Or download from: \url{https://github.com/skoulouzis/MapReduceAssignment14-15/archive/master.zip}. 
%  The dataset you will be analyzing can be found on blackboard.
  
  \section{Tag Counter}

  First, you have to find the top ten tags in the dataset. You should do the following:
  
  \begin{enumerate}
	  \item Work out how to provide entire tweets (including all the associated information) as input to your mapper.
		  Take a look at the format of the example dataset file, and think about what you could set the
		  \texttt{textinputformat.record.delimiter} configuration option to, to make MapReduce split up the input appropriately.
    \item In your \texttt{Mapper} class, extract the text of each tweet, and detect if a line contains any hash tags.
    \item If the line contains one or more hash tags, emit them to the reducers using the appropriate keys/values.
    \item In your \texttt{Reducer} class, count the number of hash tags.
    \item Afterwards, examine the final output, and determine which hash tags are the most popular.\\
    \textbf{Attention:} You shouldn't implement anything else in MapReduce (e.g. a sorting algorithm) to get the top ten tags. You can simply write a script or some code which reads the output files, after you downloaded the results. (Why don't you have to merge the output files for this?).
  \end{enumerate}

  When you're done, it might be a good idea to make a copy of your code, before you move onto the next task.

  \subsection{Sentiment Analysis}
  For the second part of the assignment, you will need to further modify both the \texttt{Mapper} and \texttt{Reduce} classes.

  In natural language processing, language identification is the problem of determining which natural language is used in a text, while sentiment analysis aims to determine the attitude of a person according the her/his writings.   
  
  To perform language detection, you will use the JLangDetect API (\url{https://github.com/melix/jlangdetect}). Import the UberLanguageDetector package: 
  
  \lstset{language=Java}      
  \begin{lstlisting}
    import me.champeau.ld.UberLanguageDetector;
  \end{lstlisting}
  
  Then, to detect the language of a String variable named \texttt{text}, you can use: 
  
  \begin{lstlisting}
    String lang = UberLanguageDetector.getInstance().detectLang(text);
  \end{lstlisting}
  
  If the language in the variable \texttt{text} is in English, the \texttt{lang} variable will be set to \texttt{'en'}. 
  
  To perform sentiment analysis on English text, you will use the Stanford Natural Language Processing API (\url{http://nlp.stanford.edu/}). To use this API you'll need to import the necessary packages in your class: 
  
  \begin{lstlisting}
    import edu.stanford.nlp.ling.CoreAnnotations;
    import edu.stanford.nlp.pipeline.Annotation;
    import edu.stanford.nlp.pipeline.StanfordCoreNLP;
    import edu.stanford.nlp.rnn.RNNCoreAnnotations;
    import edu.stanford.nlp.sentiment.SentimentCoreAnnotations;
    import edu.stanford.nlp.trees.Tree;
    import edu.stanford.nlp.util.CoreMap;
  \end{lstlisting}
  
  To detect the sentiment of a String variable named \texttt{text}, you can use the following method:
  \begin{lstlisting}
    private int findSentiment(String text) {
      Properties props = new Properties();
      props.setProperty("annotators", "tokenize, ssplit, parse, sentiment");
      props.put("parse.model", parseModelPath);
      props.put("sentiment.model", sentimentModelPath);
      StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
      
      int mainSentiment = 0;
      if (text != null && text.length() > 0) {
	int longest = 0;
	Annotation annotation = pipeline.process(text);
	for (CoreMap sentence : annotation
	.get(CoreAnnotations.SentencesAnnotation.class)) {
	  Tree tree = sentence
	  .get(SentimentCoreAnnotations.AnnotatedTree.class);
	  int sentiment = RNNCoreAnnotations.getPredictedClass(tree);
	  String partText = sentence.toString();
	  if (partText.length() > longest) {
	    mainSentiment = sentiment;
	    longest = partText.length();
	  }	  
	}
      }
      
      //This method is very demanding so try to save some memory.
      pipeline = null;
      System.gc();

      return mainSentiment;
    }
  \end{lstlisting}
  
  This method returns an integer that reflects the sentiment of a text. The higher the number, the more positive the sentiment of the text.
  
  Importantly, in this method, you need to specify the \texttt{parseModelPath} and \texttt{sentimentModelPath} variables. The first variable
  gives the path to the \texttt{englishPCFG.ser.gz} file, which is used to perform parsing of English-language text. The second gives the
  path to the \texttt{sentiment.ser.gz} file, used for sentiment detection.

  For local testing/use, you can simply download these files from Blackboard. However, on the cluster, you'll need to make sure that the files
  are available to all the mappers. We've already put them on the HDFS, in \texttt{/projects/cpp2015}.

  To make them available, you just have to use the \texttt{DistributedCache}, which is provided by the MapReduce framework to cache files (text, archives, etc) needed by applications. To use it, you have to add the files you need to the cache before running your job:

\begin{lstlisting}
DistributedCache.addCacheFile(new URI("/projects/cpp2015/sentiment.ser.gz"), job);
\end{lstlisting}

  Once you've done that, the files you've cached will be available in the directory which your mapper runs in (so you can just open them
  by their filename).

  To summarize, to complete this assignment, you'll need to:
  
  \begin{enumerate}
    \item Add the \texttt{englishPCFG.ser.gz} and \texttt{sentiment.ser.gz} files to the list of cached files.
%    \item Override the \texttt{configure()} method in the \texttt{map} class and get the local path for the \texttt{englishPCFG.ser.gz}  and \texttt{sentiment.ser.gz} files
    \item Use the mappers to detect if a tweet contains any hash tags, and if it does, detect the language of the tweet.
    \item If the language is English, perform the sentiment analysis, and emit appropriate keys and values to the reducers.
    \item Finally, use the reducers to calculate the average (mean) sentiment and the standard deviation, for each hash tag.
  \end{enumerate}
  
  The standard deviation can be iteratively (value-by-value) calculated like this: 
  \begin{lstlisting}
    sd = sd + Math.pow(val - mean, 2);
  \end{lstlisting}
  
  \section{Retweets and Replies}

  For the final part of the assignment, you need to investigate the main two types of
  engagement on Twitter: retweets, and replies. Remember, the dataset is from 2009,
  so no metadata is available for this, only the message text. \\

  Retweets: When people retweet (using RT), it usually means they think the content of the tweet is interesting or useful,
  and so they think their followers would also benefit from seeing it. By retweeting, they say ``take a look,''  and expose
  the tweet to all their followers, giving the tweet a greater reach.
  
  Reply: When a tweet is replied to (using @), it suggests that the tweet has
  resonated enough with someone that it sparks a conversation, or encourages
  someone to share it with their followers. \\

  Given the popularity of these two activities, it would be interesting to explore how they are used. You should write code, using MapReduce,
  which answers the following questions:

  \begin{itemize}
  \item How many tweets are there for each type of reaction (retweets, and replies)?
  \item When do most retweets happen (what times of day), and when do most replies happen? Is this different for different languages?
  \item What are the top 10 retweeted messages?
  \item Do those top 10 retweeted messages get significantly more replies?
  \item Are tweets with a certain sentiment more likely to generate more retweets?
  \end{itemize}

  You'll probably want to work on a separate copy of your code for this; remember to keep a copy of your code from the other tasks).

  Think about how you can implement this efficiently! You can consider things like running another MapReduce job, on the output of your first one.

  \pagebreak
  \section{Report} \label{sec:report}

  You are required to hand in both a report and your code:
  
  \begin{enumerate}
    \item A short report, of no more than 3 pages.
    \item Your source code, for all of the three tasks. You can hand in one version of the code for each task, or (ideally) one single piece of code that implements all three parts of the analysis. Please only hand in the \emph{source code}. We don't want your \texttt{target} folder or your output files, but your source code \textbf{must} be complete.
  \end{enumerate}

  Your source code should be comprehensively commented, so that we can understand exactly what you're doing. Assume that we already know how
  MapReduce works, but you should still explain how your code interacts with MapReduce in general.
  
  Your report should contain the following sections:
  
  \begin{enumerate}
    \item Introduction: Very briefly (no more that 2 paragraphs), describe the major components of MapReduce with Hadoop.
    \item Implementation: Describe your approach for implementing each part of the Twitter dataset analysis, from a high-level point of view.
    \item Results: 
    \begin{enumerate}
      \item A table with the top ten hash tags that you found, their occurrences, their average sentiment and their standard deviation.
      \item A graph showing speedup measurements when using 1, 2, 4 and 8 mappers, for at least the sentiment analysis task.
      \item Tables or graphs which present your answers to the questions about retweets and replies.
    \end{enumerate}
    \item Conclusion: 
    \begin{enumerate}
      \item Discuss your results!
      \item Make sure to talk about the benefit, if any, of adding more mappers.
	      (If you didn't observe any speed up by adding more mappers, explain why!)
      \item Explain what parts of this problem you think are well-suited to Hadoop/MapReduce, and which (if any) are not.
    \end{enumerate}
    
  \end{enumerate}
  
  
  \subsection{Experiments}
  Since you are sharing the cluster, it is possible that your measurements may be influenced by other activities.
  With this in mind, when you measure speedup, you should be sure to perform multiple experiments, and present your findings in a statistically-sound manner.
  Be sure to graph the speedup using error bars.
  % they know this from earlier assignments
  %Speedup is a metric that shows how much faster an algorithm runs if we add more instances to execute it.
  Remember, you can calculate speedup (the relative performance improvement when executing a task) using:
  
  \[
  S_p = \frac{T_1}{T_p}
  \]
  
  Where $T_1$ is the execution time when using a single mapper, and $T_p$ is the execution time when using $p$ mappers.
  
  \section{Grading}
  Deadline: See Blackboard. \\
  Grading: Tag Counter: 4/10, Sentiment Analysis: 3/10, Retweets and Replies: 3/10.  \\
  To obtain full points, your code must work correctly, be well-commented and tidy, and produce correct results. In your report, you will have to include all the requested graphs and results, and provide everything requested in Section \ref{sec:report}.
\end{document}     
